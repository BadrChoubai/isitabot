{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_pretrained_bert import (GPT2LMHeadModel, GPT2Tokenizer, BertForMaskedLM, TransfoXLTokenizer, TransfoXLLMHeadModel, OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
    "\n",
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM():\n",
    "\n",
    "    def gen_tensor(self, in_text):\n",
    "        pass\n",
    "\n",
    "    def check_probabilities(self, in_text):\n",
    "        '''\n",
    "        Function that GLTR interacts with to check the probabilities of words\n",
    "\n",
    "        Params:\n",
    "        - in_text: str -- The text that you want to check\n",
    "        - topk: int -- Your desired truncation of the head of the distribution\n",
    "\n",
    "        Output:\n",
    "        - payload: dict -- The wrapper for results in this function, described below\n",
    "\n",
    "        Payload values\n",
    "        ==============\n",
    "        real_topk: list of tuples -- (ranking, prob) of each token\n",
    "        '''\n",
    "\n",
    "        context = self.gen_tensor(in_text)\n",
    "        \n",
    "        # Forward through the model\n",
    "        # logits, _ = self.model(context) #gpt2\n",
    "        # logits = self.model(context, masked_lm_labels=None) #bert # run our tensor through model 'self.model' output is list of list of logits for each word. each list of probabilities is the length of tokenized words\n",
    "        logits = self.model(context)\n",
    "        print(logits)\n",
    "\n",
    "        # construct target and pred\n",
    "        yhat = torch.softmax(logits[0, 1:-1], dim=-1)    #softmax on first column and drop last element (end of text token's logit array)\n",
    "        \n",
    "        print(\"max prob of ind 0: \", yhat[0][1012])\n",
    "        print(\"max prob of second to last: \", yhat[0][517])\n",
    "\n",
    "\n",
    "        y = context[0, 1:-1]  #tensor without end of text token, essentially massive list of tokens representing words\n",
    "        \n",
    "\n",
    "        # Sort the predictions for each timestep\n",
    "        sorted_preds = np.argsort(-yhat.data.cpu().numpy())     #creates an array of the indexes corresponding to the yhat probabilities if they were sorted greatest to least\n",
    "\n",
    "        print(sorted_preds)\n",
    "\n",
    "        # [pos, ...]\n",
    "        real_topk_pos = list(\n",
    "            [int(np.where(sorted_preds[i] == y[i].item())[0][0])\n",
    "             for i in range(y.shape[0])])   #create vector of which place the word in text was in the returned predicted probability of every word to follow. if word was the 10th most likely, put 10 in vector\n",
    "        \n",
    "        print(real_topk_pos)\n",
    "\n",
    "        real_topk_probs = yhat[np.arange(0, y.shape[0], 1), y].data.cpu().numpy().tolist()  #make list of probabilities of each of the tokens at each index\n",
    "\n",
    "\n",
    "        real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))     #round off decimals\n",
    "\n",
    "        # create [(pos, prob), ...] from [pos, ...] and [prob, ...]\n",
    "        real_topk = list(zip(real_topk_pos, real_topk_probs))       #zip position in probability list and proabilities together\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return real_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_BERT(LM):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        In the subclass, you need to load all necessary components\n",
    "        for the other functions.\n",
    "        Typically, this will comprise a tokenizer and a model.\n",
    "        '''\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super(LM, self).__init__()\n",
    "        self.model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "        self.enc = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.start_token = \"[CLS]\"\n",
    "        self.end_token = \"[SEP]\"\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        print(\"Loaded BERT model!\")\n",
    "\n",
    "    def gen_tensor(self, in_text):\n",
    "        # Process input\n",
    "\n",
    "        # print(self.enc.encode(in_text))\n",
    "\n",
    "        start_t = torch.full((1, 1),\n",
    "                             self.enc.encode(self.start_token)[0],\n",
    "                             device=self.device,\n",
    "                             dtype=torch.long)  #create a tensor to stick words in and feed to model\n",
    "\n",
    "        end_t = torch.full((1, 1),\n",
    "                            self.enc.encode(self.end_token)[0],\n",
    "                            device=self.device,\n",
    "                            dtype=torch.long)        #create tensor with end_token\n",
    "\n",
    "        context = self.enc.encode(in_text)\n",
    "\n",
    "        context = torch.tensor(context,\n",
    "                               device=self.device,\n",
    "                               dtype=torch.long).unsqueeze(0)   #encode then add words into tensor\n",
    "        return torch.cat([start_t, context, end_t], dim=1)  #join tensors, endoftext at beginning\n",
    "\n",
    "class LM_GPT2(LM):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        In the subclass, you need to load all necessary components\n",
    "        for the other functions.\n",
    "        Typically, this will comprise a tokenizer and a model.\n",
    "        '''\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super(LM, self).__init__()\n",
    "        self.enc = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.start_token = '<|endoftext|>'\n",
    "        self.end_token = self.start_token\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        print(\"Loaded GPT-2 model!\")\n",
    "    \n",
    "    def gen_tensor(self, in_text):\n",
    "        # Process input\n",
    "        start_t = torch.full((1, 1),\n",
    "                             self.enc.encoder[self.start_token],\n",
    "                             device=self.device,\n",
    "                             dtype=torch.long)  #create a tensor to stick words in and feed to model\n",
    "\n",
    "        # print(self.enc.encoder[self.start_token])\n",
    "\n",
    "        end_t = torch.full((1, 1),\n",
    "                            self.enc.encoder[self.end_token],\n",
    "                            device=self.device,\n",
    "                            dtype=torch.long)        #create tensor with end_token\n",
    "\n",
    "        context = self.enc.encode(in_text)\n",
    "\n",
    "        context = torch.tensor(context,\n",
    "                               device=self.device,\n",
    "                               dtype=torch.long).unsqueeze(0)   #encode then add words into tensor\n",
    "        return torch.cat([start_t, context, end_t], dim=1)  #join tensors, endoftext at beginning\n",
    "\n",
    "class LM_TRANXL(LM):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        In the subclass, you need to load all necessary components\n",
    "        for the other functions.\n",
    "        Typically, this will comprise a tokenizer and a model.\n",
    "        '''\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super(LM, self).__init__()\n",
    "        self.enc = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "        self.model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')\n",
    "        self.start_token = '<|endoftext|>'\n",
    "        self.end_token = self.start_token\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        print(\"Loaded GPT-2 model!\")\n",
    "    \n",
    "    def gen_tensor(self, in_text):\n",
    "        # Process input\n",
    "        # start_t = torch.full((1, 1),\n",
    "        #                      self.enc.encoder[self.start_token],\n",
    "        #                      device=self.device,\n",
    "        #                      dtype=torch.long)  #create a tensor to stick words in and feed to model\n",
    "\n",
    "        # print(self.enc.encoder[self.start_token])\n",
    "\n",
    "        # end_t = torch.full((1, 1),\n",
    "        #                     self.enc.encoder[self.end_token],\n",
    "        #                     device=self.device,\n",
    "        #                     dtype=torch.long)        #create tensor with end_token\n",
    "\n",
    "        context = self.enc.encode(in_text)\n",
    "\n",
    "        context = torch.tensor(context,\n",
    "                               device=self.device,\n",
    "                               dtype=torch.long).unsqueeze(0)   #encode then add words into tensor\n",
    "        # return torch.cat([start_t, context, end_t], dim=1)  #join tensors, endoftext at beginning\n",
    "        return context\n",
    "\n",
    "class LM_GPT(LM):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        In the subclass, you need to load all necessary components\n",
    "        for the other functions.\n",
    "        Typically, this will comprise a tokenizer and a model.\n",
    "        '''\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super(LM, self).__init__()\n",
    "        self.enc = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "        self.model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "        self.start_token = '<|endoftext|>'\n",
    "        self.end_token = self.start_token\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        print(\"Loaded GPT-2 model!\")\n",
    "    \n",
    "    def gen_tensor(self, in_text):\n",
    "        # Process input\n",
    "        # start_t = torch.full((1, 1),\n",
    "        #                      self.enc.encoder[self.start_token],\n",
    "        #                      device=self.device,\n",
    "        #                      dtype=torch.long)  #create a tensor to stick words in and feed to model\n",
    "\n",
    "        # print(self.enc.encoder[self.start_token])\n",
    "\n",
    "        # end_t = torch.full((1, 1),\n",
    "        #                     self.enc.encoder[self.end_token],\n",
    "        #                     device=self.device,\n",
    "        #                     dtype=torch.long)        #create tensor with end_token\n",
    "\n",
    "        context = self.enc.encode(in_text)\n",
    "\n",
    "        context = torch.tensor(context,\n",
    "                               device=self.device,\n",
    "                               dtype=torch.long).unsqueeze(0)   #encode then add words into tensor\n",
    "        # return torch.cat([start_t, context, end_t], dim=1)  #join tensors, endoftext at beginning\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_code(raw_text):\n",
    "    # lm = LM_GPT2()\n",
    "    # lm = LM_BERT()\n",
    "    # lm = LM_TRANXL()\n",
    "    lm = LM_GPT()\n",
    "    start = time.time()\n",
    "    real_topk = lm.check_probabilities(raw_text)\n",
    "    end = time.time()\n",
    "    ranks = [i[0] for i in real_topk]\n",
    "    preds = [i[1] for i in real_topk]\n",
    "    print(real_topk)\n",
    "    print(\"{:.2f} Seconds for a check with GPT-2\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2 Results:\n",
    "[(11296, 0.0), (126, 0.00018), (327, 0.00036), (15, 0.003), (151, 6e-05), (737, 7e-05), (708, 0.00028), (57, 0.00159), (577, 8e-05), (1048, 5e-05), (631, 8e-05), (192, 0.00012), (257, 0.00055), (27, 0.00159), (316, 0.00012), (568, 0.00014), (63, 0.0008), (1402, 2e-05), (2677, 0.0), (336, 0.00026), (1200, 1e-05), (284, 9e-05), (937, 2e-05), (150, 0.00017), (355, 9e-05), (332, 6e-05), (965, 6e-05), (153, 1e-05), (277, 7e-05), (249, 0.00039), (99, 0.00045), (511, 6e-05), (239, 0.0004), (4856, 0.0), (642, 0.0), (901, 0.0), (211, 0.0), (185, 0.00021), (298, 0.00048), (530, 0.00024), (1837, 2e-05), (819, 0.00015), (1385, 5e-05), (70, 0.00199), (55, 0.00044), (1122, 2e-05), (12806, 0.0), (50, 0.00227), (226, 0.00026), (150, 3e-05), (326, 0.00018), (2916, 1e-05), (1407, 4e-05), (91, 8e-05), (598, 0.00017), (875, 2e-05), (1647, 1e-05)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT Results: [(409, 2e-05), (5099, 2e-05), (914, 0.0), (1294, 1e-05), (6321, 1e-05), (19, 0.00335), (1635, 2e-05), (1509, 2e-05), (1175, 4e-05), (213, 6e-05), (2725, 3e-05), (5, 0.0066), (1055, 8e-05), (1067, 0.00011), (119, 0.00024), (129, 0.00035), (62, 0.00033), (3652, 3e-05), (786, 2e-05), (1214, 6e-05), (2352, 1e-05), (140, 0.00015), (2089, 2e-05), (93, 0.00055), (5465, 0.0), (56, 9e-05), (270, 9e-05), (1510, 7e-05), (127, 0.00054), (957, 4e-05), (1924, 2e-05), (443, 0.00023), (267, 1e-05), (14, 1e-05), (435, 0.00011), (1818, 5e-05), (124, 0.00111), (492, 0.0001), (192, 0.00065), (1354, 3e-05), (17, 0.00178), (251, 3e-05), (177, 0.00025), (285, 6e-05), (16, 0.0061), (636, 0.00011), (176, 0.00015), (1128, 5e-05), (4150, 1e-05), (987, 0.00011), (720, 0.0), (549, 0.00011), (944, 1e-05)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT softmax'd logits:\n",
    "\n",
    "tensor([[1.0506e-07, 1.1879e-07, 1.1111e-07,  ..., 1.7469e-07, 2.5401e-07,\n",
    "         1.5148e-06],\n",
    "        [1.0723e-15, 3.0842e-15, 9.0170e-16,  ..., 1.6667e-14, 4.4826e-14,\n",
    "         1.9831e-13],\n",
    "        [1.5042e-10, 1.5468e-10, 8.2493e-11,  ..., 5.6238e-11, 3.6081e-10,\n",
    "         3.0738e-10],\n",
    "        ...,\n",
    "        [1.3800e-08, 1.1116e-08, 1.0551e-08,  ..., 1.2808e-08, 3.0984e-08,\n",
    "         3.9100e-10],\n",
    "        [1.5904e-17, 3.0755e-17, 1.3421e-17,  ..., 8.0877e-17, 8.6377e-17,\n",
    "         2.2796e-14],\n",
    "        [7.4714e-12, 8.7586e-12, 6.0399e-12,  ..., 1.3769e-11, 2.8122e-10,\n",
    "         3.9264e-09]], grad_fn=<SoftmaxBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 softmax'd logits:\n",
    "\n",
    "tensor([[1.6370e-05, 1.4382e-05, 1.4753e-07,  ..., 6.0292e-08, 2.8607e-08,\n",
    "         1.4753e-05],\n",
    "        [8.6838e-05, 1.5199e-05, 7.4894e-07,  ..., 8.9443e-08, 5.7484e-09,\n",
    "         2.5310e-05],\n",
    "        [1.4357e-06, 8.5909e-07, 8.2984e-08,  ..., 2.4151e-07, 1.5402e-08,\n",
    "         6.7930e-06],\n",
    "        ...,\n",
    "        [1.0720e-05, 5.9266e-06, 4.9893e-07,  ..., 1.6310e-07, 2.0822e-08,\n",
    "         1.9812e-05],\n",
    "        [3.6592e-04, 2.0784e-05, 6.0956e-06,  ..., 2.8720e-08, 5.3717e-08,\n",
    "         1.5519e-04],\n",
    "...\n",
    "        [4.9105e-06, 1.5616e-06, 1.6863e-04,  ..., 5.8378e-09, 4.4392e-09,\n",
    "         1.3820e-02]], grad_fn=<SoftmaxBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 815973/815973 [00:00<00:00, 2975081.73B/s]\n",
      "100%|██████████| 458495/458495 [00:00<00:00, 2086021.12B/s]\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "100%|██████████| 478750579/478750579 [00:37<00:00, 12698161.25B/s]\n",
      "100%|██████████| 656/656 [00:00<00:00, 151964.18B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT-2 model!\n",
      "tensor([[[ -8.4633,  -6.6491, -15.5382,  ...,  -8.9018, -10.4672,  -2.5969],\n",
      "         [ -9.9728,  -8.4973, -19.6219,  ...,  -9.9638, -11.8911,  -1.9545],\n",
      "         [ -7.1261,  -3.4169, -12.6065,  ..., -11.2287,  -5.5814,  -1.1759],\n",
      "         ...,\n",
      "         [ -5.9414,  -3.3718, -15.9036,  ..., -10.6002,  -7.5471,  -2.3585],\n",
      "         [ -6.7730,  -5.1471, -20.4338,  ..., -13.8276, -15.5916,  -2.4700],\n",
      "         [ -6.1527,  -4.7163, -17.3600,  ...,  -9.7287, -11.0065,   4.5684]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "max prob of ind 0:  tensor(1.6933e-08, grad_fn=<SelectBackward0>)\n",
      "max prob of second to last:  tensor(2.3744e-06, grad_fn=<SelectBackward0>)\n",
      "[[  504   481   551 ... 32953  4471  8225]\n",
      " [  720  4786  1353 ... 21529  5818  4118]\n",
      " [  498   488   239 ... 27888 16728 16481]\n",
      " ...\n",
      " [  239   240   488 ... 34610 22079 25910]\n",
      " [  481   246  1183 ... 37510 21210 16024]\n",
      " [  239   240   485 ... 12319 16481 22079]]\n",
      "[409, 5099, 914, 1294, 6321, 19, 1635, 1509, 1175, 213, 2725, 5, 1055, 1067, 119, 129, 62, 3652, 786, 1214, 2352, 140, 2089, 93, 5465, 56, 270, 1510, 127, 957, 1924, 443, 267, 14, 435, 1818, 124, 492, 192, 1354, 17, 251, 177, 285, 16, 636, 176, 1128, 4150, 987, 720, 549, 944]\n",
      "[(409, 2e-05), (5099, 2e-05), (914, 0.0), (1294, 1e-05), (6321, 1e-05), (19, 0.00335), (1635, 2e-05), (1509, 2e-05), (1175, 4e-05), (213, 6e-05), (2725, 3e-05), (5, 0.0066), (1055, 8e-05), (1067, 0.00011), (119, 0.00024), (129, 0.00035), (62, 0.00033), (3652, 3e-05), (786, 2e-05), (1214, 6e-05), (2352, 1e-05), (140, 0.00015), (2089, 2e-05), (93, 0.00055), (5465, 0.0), (56, 9e-05), (270, 9e-05), (1510, 7e-05), (127, 0.00054), (957, 4e-05), (1924, 2e-05), (443, 0.00023), (267, 1e-05), (14, 1e-05), (435, 0.00011), (1818, 5e-05), (124, 0.00111), (492, 0.0001), (192, 0.00065), (1354, 3e-05), (17, 0.00178), (251, 3e-05), (177, 0.00025), (285, 6e-05), (16, 0.0061), (636, 0.00011), (176, 0.00015), (1128, 5e-05), (4150, 1e-05), (987, 0.00011), (720, 0.0), (549, 0.00011), (944, 1e-05)]\n",
      "0.40 Seconds for a check with GPT-2\n"
     ]
    }
   ],
   "source": [
    "txt = \"To check the smoothness of a text, I will plot the rank of every word. If the ranks of words in a text are higher, the text will be unsmooth according to the GPT-2 Language Model. Following code is used to create these plots for texts.\"\n",
    "# print(len(txt.split()))\n",
    "main_code(txt)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afed171598be429a3ff25267819ad9841fe6a9d58681b12fb6fef5d39e1931f8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
