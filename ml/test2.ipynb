{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from pytorch_pretrained_bert import (GPT2LMHeadModel, GPT2Tokenizer, BertTokenizer, BertForMaskedLM)\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractLanguageChecker():\n",
    "    \"\"\"\n",
    "    Abstract Class that defines the Backend API of GLTR.\n",
    "\n",
    "    To extend the GLTR interface, you need to inherit this and\n",
    "    fill in the defined functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        In the subclass, you need to load all necessary components\n",
    "        for the other functions.\n",
    "        Typically, this will comprise a tokenizer and a model.\n",
    "        '''\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def check_probabilities(self, in_text, topk=40):\n",
    "        '''\n",
    "        Function that GLTR interacts with to check the probabilities of words\n",
    "\n",
    "        Params:\n",
    "        - in_text: str -- The text that you want to check\n",
    "        - topk: int -- Your desired truncation of the head of the distribution\n",
    "\n",
    "        Output:\n",
    "        - payload: dict -- The wrapper for results in this function, described below\n",
    "\n",
    "        Payload values\n",
    "        ==============\n",
    "        bpe_strings: list of str -- Each individual token in the text\n",
    "        real_topk: list of tuples -- (ranking, prob) of each token\n",
    "        pred_topk: list of list of tuple -- (word, prob) for all topk\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def postprocess(self, token):\n",
    "        \"\"\"\n",
    "        clean up the tokens from any special chars and encode\n",
    "        leading space by UTF-8 code '\\u0120', linebreak with UTF-8 code 266 '\\u010A'\n",
    "        :param token:  str -- raw token text\n",
    "        :return: str -- cleaned and re-encoded token text\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    '''\n",
    "    Filters logits to only the top k choices\n",
    "    from https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
    "    '''\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1]\n",
    "    return torch.where(logits < min_values,\n",
    "                       torch.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "                       logits)\n",
    "\n",
    "\n",
    "\n",
    "class LM(AbstractLanguageChecker):\n",
    "    def __init__(self, model_name_or_path=\"gpt2\"):\n",
    "        super(LM, self).__init__()\n",
    "        self.enc = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.start_token = '<|endoftext|>'\n",
    "        print(\"Loaded GPT-2 model!\")\n",
    "\n",
    "    def check_probabilities(self, in_text, topk=40):\n",
    "        # Process input\n",
    "        start_t = torch.full((1, 1),\n",
    "                             self.enc.encoder[self.start_token],\n",
    "                             device=self.device,\n",
    "                             dtype=torch.long)\n",
    "        context = self.enc.encode(in_text)\n",
    "        context = torch.tensor(context,\n",
    "                               device=self.device,\n",
    "                               dtype=torch.long).unsqueeze(0)\n",
    "        context = torch.cat([start_t, context], dim=1)\n",
    "        # Forward through the model\n",
    "        logits, _ = self.model(context)\n",
    "\n",
    "        # construct target and pred\n",
    "        yhat = torch.softmax(logits[0, :-1], dim=-1)\n",
    "        y = context[0, 1:]\n",
    "        # Sort the predictions for each timestep\n",
    "        sorted_preds = np.argsort(-yhat.data.cpu().numpy())\n",
    "        # [(pos, prob), ...]\n",
    "        real_topk_pos = list(\n",
    "            [int(np.where(sorted_preds[i] == y[i].item())[0][0])\n",
    "             for i in range(y.shape[0])])\n",
    "        real_topk_probs = yhat[np.arange(\n",
    "            0, y.shape[0], 1), y].data.cpu().numpy().tolist()\n",
    "        real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))\n",
    "\n",
    "        real_topk = list(zip(real_topk_pos, real_topk_probs))\n",
    "        # [str, str, ...]\n",
    "        bpe_strings = [self.enc.decoder[s.item()] for s in context[0]]\n",
    "\n",
    "        bpe_strings = [self.postprocess(s) for s in bpe_strings]\n",
    "\n",
    "        # [[(pos, prob), ...], [(pos, prob), ..], ...]\n",
    "        pred_topk = [\n",
    "            list(zip([self.enc.decoder[p] for p in sorted_preds[i][:topk]],\n",
    "                     list(map(lambda x: round(x, 5),\n",
    "                              yhat[i][sorted_preds[i][\n",
    "                                      :topk]].data.cpu().numpy().tolist()))))\n",
    "            for i in range(y.shape[0])]\n",
    "\n",
    "        pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]\n",
    "        payload = {'bpe_strings': bpe_strings,\n",
    "                   'real_topk': real_topk,\n",
    "                   'pred_topk': pred_topk}\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return payload\n",
    "\n",
    "    def sample_unconditional(self, length=100, topk=5, temperature=1.0):\n",
    "        '''\n",
    "        Sample `length` words from the model.\n",
    "        Code strongly inspired by\n",
    "        https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
    "\n",
    "        '''\n",
    "        context = torch.full((1, 1),\n",
    "                             self.enc.encoder[self.start_token],\n",
    "                             device=self.device,\n",
    "                             dtype=torch.long)\n",
    "        prev = context\n",
    "        output = context\n",
    "        past = None\n",
    "        # Forward through the model\n",
    "        with torch.no_grad():\n",
    "            for i in range(length):\n",
    "                logits, past = self.model(prev, past=past)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                # Filter predictions to topk and softmax\n",
    "                probs = torch.softmax(top_k_logits(logits, k=topk),\n",
    "                                      dim=-1)\n",
    "                # Sample\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "                # Construct output\n",
    "                output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "        output_text = self.enc.decode(output[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    def postprocess(self, token):\n",
    "        with_space = False\n",
    "        with_break = False\n",
    "        if token.startswith('Ġ'):\n",
    "            with_space = True\n",
    "            token = token[1:]\n",
    "            # print(token)\n",
    "        elif token.startswith('â'):\n",
    "            token = ' '\n",
    "        elif token.startswith('Ċ'):\n",
    "            token = ' '\n",
    "            with_break = True\n",
    "\n",
    "        token = '-' if token.startswith('â') else token\n",
    "        token = '“' if token.startswith('ľ') else token\n",
    "        token = '”' if token.startswith('Ŀ') else token\n",
    "        token = \"'\" if token.startswith('Ļ') else token\n",
    "\n",
    "        if with_space:\n",
    "            token = '\\u0120' + token\n",
    "        if with_break:\n",
    "            token = '\\u010A' + token\n",
    "\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text(vals, what, name):\n",
    "    if what==\"prob\":\n",
    "        ourvals = vals[0]\n",
    "        x = list(range(1,len(ourvals)+1))\n",
    "        y = ourvals\n",
    "        plt.plot(x, y, color='orange')\n",
    "        plt.ylim(0,1)\n",
    "        plt.savefig(name + \".png\")\n",
    "        # plt.show()\n",
    "    elif what==\"rank\":\n",
    "        ourvals = vals[1]\n",
    "        x = list(range(1, len(ourvals) + 1))\n",
    "        y = ourvals\n",
    "        plt.plot(x, y, color='orange')\n",
    "        plt.ylim(-1000, 50000)\n",
    "        plt.savefig(name + \".png\")\n",
    "        # plt.show()def main_code(raw_text):\n",
    "\n",
    "    lm = LM()\n",
    "    start = time.time()\n",
    "    payload = lm.check_probabilities(raw_text, topk=5)\n",
    "    # print(payload[\"pred_topk\"])\n",
    "    real_topK = payload[\"real_topk\"]\n",
    "    ranks = [i[0] for i in real_topK]\n",
    "    preds = [i[1] for i in real_topK]\n",
    "    plot_text([preds, ranks], 'rank', \"rank_\")\n",
    "    end = time.time()\n",
    "    print(\"{:.2f} Seconds for a check with GPT-2\".format(end - start))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afed171598be429a3ff25267819ad9841fe6a9d58681b12fb6fef5d39e1931f8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
